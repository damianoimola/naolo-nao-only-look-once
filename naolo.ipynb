{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOvJy2pp/U8pFK2B3HogYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianoimola/naolo-nao-only-look-once/blob/master/naolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0xGSW7Fj8BR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Convolutions paper: https://arxiv.org/pdf/1912.03458\n",
        "\n",
        "Awesome Dynamic Convolutions: https://github.com/kaijieshi7/awesome-dynamic-convolution"
      ],
      "metadata": {
        "id": "WgD4gP94lykA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"attention layer ad described in the paper\"\"\"\n",
        "    def __init__(self, c_dim, hidden_dim, nof_kernels):\n",
        "        super().__init__()\n",
        "        # global average pooling layer\n",
        "        self.global_pooling = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.to_scores = nn.Sequential(\n",
        "            nn.Linear(c_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, nof_kernels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, temperature=1):\n",
        "        out = self.global_pooling(x)\n",
        "        scores = self.to_scores(out)\n",
        "        return F.softmax(scores / temperature, dim=-1)"
      ],
      "metadata": {
        "id": "tyBZLB-klg8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicConvolution(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nof_kernels, reduce, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        \"\"\"Dynamic convolution layer as written in the paper\"\"\"\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        # control whether in_c and out_c are divisible by groups (i.e. the connection between inputs and outputs)\n",
        "        assert in_channels % groups == 0 and out_channels % groups == 0\n",
        "\n",
        "        self.groups = groups\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.nof_kernels = nof_kernels\n",
        "        self.attention = AttentionLayer(in_channels, max(1, in_channels // reduce), nof_kernels)\n",
        "        self.kernel_size = _pair(kernel_size)\n",
        "        self.kernels_weights = nn.Parameter(torch.Tensor(nof_kernels, out_channels, in_channels // self.groups, *self.kernel_size), requires_grad=True)\n",
        "\n",
        "        if bias:\n",
        "            self.kernels_bias = nn.Parameter(torch.Tensor(nof_kernels, out_channels), requires_grad=True)\n",
        "        else:\n",
        "            # register bias parameter in class' memory\n",
        "            self.register_parameter('kernels_bias', None)\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        # Kaiming uniform initialization\n",
        "        for i_kernel in range(self.nof_kernels):\n",
        "            init.kaiming_uniform_(self.kernels_weights[i_kernel], a=math.sqrt(5))\n",
        "\n",
        "        # uniform bias initialization (if present)\n",
        "        if self.kernels_bias is not None:\n",
        "            bound = 1 / math.sqrt(self.kernels_weights[0, 0].numel())\n",
        "            nn.init.uniform_(self.kernels_bias, -bound, bound)\n",
        "\n",
        "    def forward(self, x, temperature=1):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # attention computation\n",
        "        # [batch_size, nof_kernels]\n",
        "        alphas = self.attention(x, temperature)\n",
        "\n",
        "        # combining kernels using the scores (i.e. dynamic kernel aggregation)\n",
        "        # [batch_size, out_C, in_C/groups, kernel_height, kernel_width]\n",
        "        agg_weights = torch.sum(\n",
        "            torch.mul(self.kernels_weights.unsqueeze(0), alphas.view(batch_size, -1, 1, 1, 1, 1)),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # reshape for grouped convolution\n",
        "        # [batch_size*out_C, in_C, kernel_size, kernel_size]\n",
        "        agg_weights = agg_weights.view(-1, *agg_weights.shape[-3:])\n",
        "\n",
        "        # the very same we did just now but for biases\n",
        "        if self.kernels_bias is not None:\n",
        "            agg_bias = torch.sum(torch.mul(self.kernels_bias.unsqueeze(0), alphas.view(batch_size, -1, 1)), dim=1)\n",
        "            agg_bias = agg_bias.view(-1)\n",
        "        else:\n",
        "            agg_bias = None\n",
        "\n",
        "        # reshape for grouped convolutions\n",
        "        # [1, batch_size*out_C, H, W]\n",
        "        x_grouped = x.contiguous().view(1, batch_size*self.out_channels, *x.shape[-2:])\n",
        "\n",
        "        # grouped convolution\n",
        "        # [1, batch_size*out_C, H', W']\n",
        "        out = F.conv2d(x_grouped, agg_weights, agg_bias, groups=self.groups*batch_size,\n",
        "                       stride=self.stride, padding=self.padding, dilation=self.dilation)\n",
        "\n",
        "        # reshape back to standard dimensions\n",
        "        # [batch_size, out_C, H', W']\n",
        "        out = out.view(batch_size, -1, *out.shape[-2:])\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "DjbrNdFPljGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== HELPER FUNCTIONS (only 3x3 used) =====\n",
        "def dconv3x3(in_channels, out_channels):\n",
        "    return DynamicConvolution(in_channels, out_channels, nof_kernels=4, reduce=1, kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1)\n",
        "\n",
        "def dconv5x5(in_channels, out_channels):\n",
        "    return DynamicConvolution(in_channels, out_channels, nof_kernels=4, reduce=1, kernel_size=5, stride=1, padding=1, groups=1, bias=False, dilation=1)\n",
        "\n",
        "def dconv7x7(in_channels, out_channels):\n",
        "    return DynamicConvolution(in_channels, out_channels, nof_kernels=4, reduce=1, kernel_size=7, stride=1, padding=1, groups=1, bias=False, dilation=1)"
      ],
      "metadata": {
        "id": "5HI10Q94ll_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastYOLO(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes=80,\n",
        "        anchors=None,\n",
        "        width_multiplier=0.5,\n",
        "        conv_layer=nn.Conv2d,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A fast, compact YOLO-like model.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of object classes.\n",
        "            anchors (list of tuples): Anchor box sizes (w, h).\n",
        "            width_multiplier (float): Factor to reduce channel widths.\n",
        "            conv_layer (nn.Module): Convolutional layer class to use (e.g., nn.Conv2d or DynamicConv2d).\n",
        "        \"\"\"\n",
        "        super(FastYOLO, self).__init__()\n",
        "        if anchors is None:\n",
        "            # Default anchors (tiny-scale)\n",
        "            anchors = [(10,13), (16,30), (33,23)]\n",
        "        self.anchors = torch.tensor(anchors, dtype=torch.float32)\n",
        "        self.num_anchors = len(anchors)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Base channel configuration (Tiny YOLO-like)\n",
        "        base_channels = [16, 32, 64, 128, 256]\n",
        "        channels = [int(c * width_multiplier) for c in base_channels]\n",
        "\n",
        "        # Construct sequential backbone\n",
        "        layers = []\n",
        "        in_ch = 3\n",
        "        for out_ch in channels:\n",
        "            layers.append(conv_layer(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "            layers.append(nn.BatchNorm2d(out_ch))\n",
        "            layers.append(nn.LeakyReLU(0.1, inplace=True))\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            in_ch = out_ch\n",
        "\n",
        "        # Final conv to produce predictions\n",
        "        # Each anchor predicts (5 + num_classes) values\n",
        "        pred_channels = self.num_anchors * (self.num_classes + 5)\n",
        "        layers.append(conv_layer(in_ch, pred_channels, kernel_size=1, stride=1, padding=0, bias=True))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Returns:\n",
        "            preds: Tensor of shape [B, num_anchors*(5+num_classes), H/32, W/32]\n",
        "            anchors: Tensor of shape [num_anchors, 2]\n",
        "        \"\"\"\n",
        "        preds = self.model(x)\n",
        "        return preds, self.anchors\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    model = FastYOLO(num_classes=20, width_multiplier=0.33, conv_layer=DynamicConv2d)\n",
        "    dummy = torch.randn(1, 3, 416, 416)\n",
        "    preds, anchors = model(dummy)\n",
        "    print(\"Predictions shape:\", preds.shape)\n",
        "    print(\"Anchors:\", anchors)\n"
      ],
      "metadata": {
        "id": "WC3BAuJTlAFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastYOLO(num_classes=20, width_multiplier=0.33, conv_layer=DynamicConv2d)\n",
        "dummy = torch.randn(1, 3, 416, 416)\n",
        "preds, anchors = model(dummy)\n",
        "print(\"Predictions shape:\", preds.shape)\n",
        "print(\"Anchors:\", anchors)"
      ],
      "metadata": {
        "id": "xhDP6V5jmVNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}